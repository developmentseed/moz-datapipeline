# Road Planning data pipeline
This repository contains the data pipeline that prepares Vector Tiles for use by the [Road Planning Tool](https://github.com/developmentseed/moz-road-planning).

## Usage
Processing all the information on the pipeline requires multiple steps, which have to run in the order outlines below.

### 1. Img Stats
TODO:
- What are they?
- Requirements? 
- How to run?

### 2. Indicators pipeline
This is the main part of the pipeline which will compute the indicators and also prepare the data needed for the next steps.
Run with:
```
docker-compose up
```
If you make a change to one of the scripts, run the following to rebuild the image and run the pipeline again:
```
docker-compose up --build
```

### 3. Script EAUL
Calculating the EAUL is very computationally expensive, so this was built as a docker container that can run in several machines at once allowing distributed computation.
For this same reason it requires some files to be uploaded to a S3 bucket.
- OD pairs - `od.geojson`
- Road network in osm xml - `roadnetwork.osm`
- Traffic data - `traffic.json`

All these files are generated in the previous step and are found inside the `output/` directory.
Once the files are uploaded the EAUL script can run. See the [EAUL script README.md](./script-eaul/README.md) for more information.

### 4. Adding EAUL results to the road network 
Once the EAUL processing finished, the results can be added to the road network. This is a relatively fast operation that can be ran locally using the same docker container that contains the pipeline.
See the [README.md](./scripts/merge-eaul/README.md) for instructions.

### 5. Generating the vector tiles
After having all the data processed, generate the vector tiles to be used by the application.
The vector tiles script uses the Road Network and the Bridges in the `output/` directory and uploads the vector tiles to the specified S3 bucket.

```
docker-compose run \
  -e AWS_ACCESS_KEY_ID='code here' \
  -e AWS_SECRET_ACCESS_KEY='secret here' \
  moz-datapipeline \
  bash scripts/vector-tiles.sh S3_bucket
```

-----

## Repository structure

- `/scripts` contains the scripts needed to produce the final Vector Tiles
- `/script-eaul` contains the EAUL script. It is separate because has different run requirements
- `/source` contains the input data for the pipeline
- `/output` contains the files generated by the pipeline

## Calculating indicators
The scripts to calculate the different indicators (poverty rate, agriculture potential, etc) are stored in the `/scripts` folder. The data pipeline is not opinionated about what language to use and depending on the indicator being calculated, scripts may be written in `node`, `python`, or `bash`. When developing new scripts, please use the following guidelines:

- the input data for the scripts is stored in the `./source` folder
- a script writes the final indicator to a CSV file in the `./output` folder. The requirements for these files are:  
  - the file has to contain two columns: `way_id` and `score`, and can contain an optional `value`. Additional columns are ignored by the data pipeline.
  - the value for `score` should be on a scale from 0 - 100.
  - each file contains the score and value for one indicator. If the script calculates multiple indicators for each road segment, these need to be stored in separate files.
  - the CSV file needs to be named after the indicator. The data pipeline will use the basename of the CSV file (without extension) as the name of the attribute in the final Vector Tiles
- temporary data can be written to the `./tmp` folder. This will be created on start of the data pipeline, and cleaned up on finish
- individual scripts may fail, but should not fail the full pipeline. If a script fails, print a friendly error message to the user so they understand which indicator is not present in the final dataset
